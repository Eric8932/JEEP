{
 "layers": [
  23
 ],
 "num_steps": 25,
 "lr": 5e-5,
 "min_loss" : 1e-1,
 "weight_decay": false,
"wd_power_law": [-0.87028798,  0.15589562],
"kl_factor": 0,
"norm_constraint": false,
 "rewrite_module_tmp": "model.layers.{}",
 "layer_module_tmp": "model.layers.{}",
 "mlp_module_tmp": "model.layers.{}.mlp",
"attn_module_tmp": "model.layers.{}.self_attn",
"ln_f_module": "model.norm",
"lm_head_module": "lm_head"
}
